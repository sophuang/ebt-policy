<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We outscaled (feed-forward) transformers and generalized reasoning/system 2 thinking to any modality and problem. This is done by training a new class of models called Energy-Based Transformers (EBTs), which are energy based models designed for scalability, parallelizability, stability, and the ability to learn to think from unsupervised learning. The results are absolutely amazing--outscaling feed-forward transformers across modalities and axes (i.e. FLOPs, parameters, depth, even data).">
  <meta property="og:title" content="Energy-Based Transformers: Outscaling Transformers and Generalizable Reasoning"/>
  <meta property="og:description" content="Learn how Energy-Based Transformers (EBTs) enable improved scalability over traditional transformers while generalizing reasoning/thinking capabilities to be learned on any problem. #AI #DeepLearning #EBMs #Transformers #reasonig #system 2 thinking"/>
  <meta property="og:url" content="https://energy-based-transformers.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Energy-Based Transformers: Outscaling Transformers and Generalizable Reasoning">
  <meta name="twitter:description" content="We outscaled (feed-forward) transformers and generalized reasoning/system 2 thinking to any modality and problem. This is done by training a new class of models called Energy-Based Transformers (EBTs), which are energy based models designed for scalability, parallelizability, stability, and the ability to learn to think from unsupervised learning. The results are absolutely amazing--outscaling feed-forward transformers across modalities and axes (i.e. FLOPs, parameters, depth, even data).">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="deep learning, ai, reasoning, system 2 thinking, scaling, energy based models, energy-based models, transformers, ebms, verification, scaling law, test-time compute, inference-time compute, cognitively inspired energy based world models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ðŸ™ƒ</text></svg>">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="module" src="https://cdn.jsdelivr.net/gh/zerodevx/zero-md@2/dist/zero-md.min.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="color: #0066cc;">EBT-Policy:</span>Energy Unlocks Emergent Physical Reasoning Capabilities</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <span class="author-block"><a href="https://nanduruganesh.github.io/" target="_blank">Travis Davies</a>,</span>
                <a href="https://alexiglad.github.io" target="_blank">Alexi Gladstone</a>,</span>
                
              </div>

              <!-- <div class="is-size-5 publication-authors">
                <span class="author-block">Institution Name<br>Conferance name and year</span>
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
              </div> -->

              <div class="column has-text-centered">
                <div class="publication-links">
                     <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a href="static/pdfs/paper.pdf" target="_blank"
                    class="button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>


                    <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2507.02092" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/alexiglad/EBT" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <!-- Blog link -->
            <span class="link-block">
              <a href="https://alexiglad.github.io/blog/2025/ebt/" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fas fa-newspaper"></i>
              </span>
              <span>Blog</span>
            </a>
          </span>

            <!-- Hugging Face link -->
            <span class="link-block">
              <a href="https://huggingface.co/papers/2507.02092" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.svg"
                       alt="Hugging Face" style="height:1em;" />
                </span>
                <span>HF Paper</span>
              </a>
            </span>

            <!-- Twitter link -->
            <span class="link-block">
              <a href="https://x.com/AlexiGlad/status/1942231878305714462" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-twitter"></i>
                </span>
                <span>Tweet</span>
              </a>
            </span>

          <!-- YouTube Video link -->
          <span class="link-block">
            <a href="https://youtu.be/RAEy3JZmIaA?si=xNPLMX0miujFQn2B" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fab fa-youtube"></i>
            </span>
            <span>Video</span>
          </a>
        </span>

            
        </div>
      </div>
    </div>
  </div>
</div>
</section>


<!-- TLDR -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          <p>
            Energy-Based Transformers (EBTs) <strong>outscale</strong> (feed-forward) transformers while <strong>generalizing</strong> reasoning/system 2 thinking to any modality/problem <strong>without</strong> requiring verifiable rewards! EBTs are the <strong>first approach</strong> to outscale feed-forward transformers across modalities and with respect to several axes including data, depth, parameters, FLOPs, etc. EBTs can also think over every single prediction being made (i.e. every token in language modeling) and <strong>generalize better</strong> than existing models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-vcentered">
        <div class="column has-text-centered">
          <img src="static/gifs/ezgif.com-animated-gif-maker_video.gif" alt="Energy Based Transformer Video Prediction as Thinking" class="teaser-gif">
        </div>
        <div class="column has-text-centered">
          <img src="static/gifs/ezgif.com-animated-gif-maker_text.gif" alt="Energy Based Transformer Language Model Prediction as Thinking" class="teaser-gif">
        </div>
      </div>
      <h2 class="subtitle has-text-centered">
        Thinking Processes visualized as energy minimization for autoregressive EBTs. Initially, a random prediction is fed into the models, causing the EBT to predict high energy. Then, models iteratively refine these predictions by minimizing the energy through gradient descent (passing the gradient from the energy to predictions). This process is performed until convergence of the energy, enabling the model to know "when to stop thinking."
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <zero-md no-shadow data-dedent="12" style="display:block;width:100%;">
            <script type="text/markdown">
Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question _"Is it possible to **generalize** these System 2 Thinking approaches, and **develop models that learn to think solely from unsupervised learning?**"_ Interestingly, we find the answer is **yes**, by learning to explicitly **verify** the compatibility (unnormalized probability) between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs)---a new class of Energy-Based Models (EBMs)---to assign an **energy** (unnormalized probability) value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. This formulation enables System 2 Thinking to emerge from unsupervised learning, making it modality and problem agnostic. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking (i.e., extra computation) by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using 99% fewer forward passes. Further, we find that System 2 Thinking with EBTs yields larger performance improvements on data that is farther out-of-distribution, and that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, enabling EBTs to out-generalize existing paradigms. Consequently, EBTs are a flexible and exciting new paradigm for scaling both the **learning** and **thinking** capabilities of models.
            </script>
          </zero-md>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">

        <img src="static/images/model_comparison.png" alt="Comparison of EBTs to existing approaches"/>
        <h2 class="subtitle has-text-centered">
          Comparison of EBTs to existing approaches
        </h2>
      </div>
      <div class="item">

        <img src="static/images/proposed_model.png" alt="EBTs for Autoregressive Modeling"/>
        <h2 class="subtitle has-text-centered">
          EBTs for Autoregressive Modeling
        </h2>
      </div>
      <div class="item">

        <img src="static/images/energy_landscape_minimization.png" alt="EBT Thinking Process Visualization for Language Modeling"/>
        <h2 class="subtitle has-text-centered">
         EBT Thinking Process Visualization for Language Modeling
       </h2>
     </div>
     <div class="item">

      <img src="static/images/thinking_performance.png" alt="Thinking Scalability of EBTs"/>
      <h2 class="subtitle has-text-centered">
        Thinking Scalability of EBTs
      </h2>
    </div>
    <div class="item">

      <img src="static/images/scaling_thinking_nlp_ar_ood.svg" alt="Thinking with EBTs Helps More on More OOD Data"/>
      <h2 class="subtitle has-text-centered">
        Thinking with EBTs Helps More on More OOD Data
      </h2>
    </div>
    <div class="item">

      <img src="static/images/scaling_nlp_1.png" alt="EBTs Outscale Transformer++ in NLP"/>
      <h2 class="subtitle has-text-centered">
        EBTs Outscale Transformer++ in NLP
      </h2>
    </div>
    <div class="item">

      <img src="static/images/scaling_video_1.png" alt="EBTs Outscale Transformer++ in Video"/>
      <h2 class="subtitle has-text-centered">
        EBTs Outscale Transformer++ in Video
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->






<!-- Youtube video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">For more information please reference the <a href="https://alexiglad.github.io/blog/2025/ebt/" target="_blank">blog post</a>, <a href="https://github.com/alexiglad/EBT" target="_blank">code</a>, or <a href="static/pdfs/paper.pdf" target="_blank">paper</a>!</h2>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Paper Presentation Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/TODO" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->









<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">
      BibTeX
      <button id="copyBibtex" class="button is-small is-dark is-pulled-right">Copy</button>
    </h2>
    <pre><code id="bibtexCitation">@misc{gladstone2025energybasedtransformersscalablelearners,
  title={Energy-Based Transformers are Scalable Learners and Thinkers},
  author={Alexi Gladstone and Ganesh Nanduru and Md Mofijul Islam and Peixuan Han and Hyeonjeong Ha and Aman Chadha and Yilun Du and Heng Ji and Jundong Li and Tariq Iqbal},
  year={2025},
  eprint={2507.02092},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2507.02092}
}</code></pre>
  </div>
</section>

<script>
document.addEventListener('DOMContentLoaded', () => {
  const btn = document.getElementById('copyBibtex')
  const code = document.getElementById('bibtexCitation')
  if (btn && code) {
    btn.addEventListener('click', () =>
      navigator.clipboard.writeText(code.innerText.trim())
    )
  }
})
</script>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
